# Detecting-Bias-in-AI-A-Case-Study-of-LLM-Generated-Diversity-Reports-for-Tech-Companies-
## Project Overview
This project aims to detect subtle and explicit forms of bias, specifically gender, religion, age, disability, and sexuality biases in AI-generated diversity reports. Using a RoBERTa-based multi-label classification model, the system was trained on over 10,000 annotated sentences derived from synthetic reports generated by GPT-4, GPT-4o, and Gemini.

Key features include:

### -Multi-label classification of five bias types at the sentence level

### -Confidence-weighted annotation scheme to improve training quality

### -Focal Loss and oversampling techniques to address class imbalance

### -Threshold tuning per label to maximize F1-score and recall

### -High-precision predictions on real-world, sentence-level input data

### -Cleaned and structured output CSV files for further analysis or integration

The final system enables ethical auditing of AI-generated content, especially in sensitive domains like diversity, equity, and inclusion (DEI) communications.

## Installation Instructions
### Pre requisites
Python 3.9+

Jupyter Notebook

pip or conda

GPU-enabled system (optional but recommended for training)

## Project Structure
| File/Notebook                            | Description                                                                           |
|------------------------------------------|---------------------------------------------------------------------------------------|
| `ReportGenerationCode-Gemini.ipynb`      | Script to generate AI diversity reports using Gemini LLM.          |
| `SentenceBySentence.ipynb`               | Converts full reports into individual sentences for sentence-level bias analysis.     |
| `CombinedAnnotationwithConfidence.ipynb` | Combines multi-annotator votes into final labels using confidence-based aggregation.  |
| `RobertaModel.ipynb`                     | RoBERTa training and fine-tuning notebook for multi-label bias classification.        |
| `Predictions_Cleaned.csv`                | Cleaned model predictions with sentence-level bias labels and confidence scores.      |
| `Prediction&Visualisation.ipynb`         | Visualizes prediction results and confidence scores across different bias categories. |
| `README.md`                              | Project overview, setup instructions, and usage guide.                                |


## Usage
### Generate Reports Using Gemini LLM
ReportGenerationCode-Gemini.ipynb can be used to auto-generate AI diversity reports using the Gemini API for synthetic data creation.

### Convert Full Reports into Sentences
Use SentenceBySentence.ipynb to split AI-generated diversity reports into individual sentences. This prepares the text for sentence-level bias prediction.

### Aggregate Annotations
Run CombinedAnnotationwithConfidence.ipynb to generate a confidence-weighted final label for each sentence. This produces a high-quality training dataset.

### Train the RoBERTa Bias Classifier
Open RobertaModel.ipynb to train or fine-tune the multi-label classifier on the annotated dataset. The model will learn to detect gender, religion, age, disability, and sexuality biases.

### Visualise Predictions and Confidence Levels
Use Prediction&Visualisation.ipynb to explore sentence-level predictions, confidence scores, and distribution of biases across reports.

## Contact and Future Works
### Contact
This project is maintained by Mathew Sunil Abraham.
For questions, collaboration, or feedback, feel free to reach out:
Email: msabraham98@gmail.com

### Future Works
This project lays the foundation for bias detection in AI-generated diversity reports using NLP. While the core model and prediction pipeline are complete, there are multiple opportunities to extend this work:

üåê App Deployment:
Deploy the model via a Streamlit or Flask app to allow real-time bias prediction from uploaded reports.

üì° API Integration:
Wrap the prediction module into a RESTful API for integration with LLM-based workflows or CMS tools.

üß™ Explainability Layer:
Add model explainability using tools like LIME or SHAP to help users understand why a sentence is flagged.

üìà Benchmarking Across LLMs:
Expand evaluation across reports generated from GPT-4o, Gemini, Claude, etc. to compare bias prevalence.

