# Detecting-Bias-in-AI-A-Case-Study-of-LLM-Generated-Diversity-Reports-for-Tech-Companies
## ğŸ“š Overview
This project investigates and detects bias in diversity reports generated by Large Language Models (LLMs) for tech companies. It focuses on identifying sentence-level biases related to gender, religion, age, disability, and sexuality using a multi-label classification approach.

The project aims to improve AI-generated reporting fairness and provide measurable bias detection methods to support ethical AI practices.

## ğŸ¯ Key Features
Bias Detection Model: Fine-tuned RoBERTa-based multi-label classifier for sentence-level bias identification.

Five Bias Categories: Gender, Religion, Age, Disability, Sexuality.

Manual Annotation Pipeline: Collaborative dataset labeling across 300+ diversity reports.

Confidence-Weighted Classification: Majority voting with confidence scores for improved model reliability.

Bias Quantification: Assigns sentence-level bias labels and generates bias distribution summaries.

## ğŸ› ï¸ Tools & Technologies
Python

RoBERTa (Hugging Face Transformers)

Pandas, NumPy, Scikit-learn

Streamlit (for deployment interface)

Jupyter Notebook (for training and analysis)

## ğŸ“ How It Works
Dataset Preparation: Generated 990 LLM-based diversity reports using GPT-4, GPT-4o, and Gemini.

Annotation: Manually labelled 300 reports (~15,000 sentences) for bias categories.

Model Training: Fine-tuned a RoBERTa classifier on the annotated dataset.

Bias Detection: Predicted sentence-level bias categories with confidence scores.

Output Generation: Visualised bias distributions and sentence classifications via Streamlit dashboard.
